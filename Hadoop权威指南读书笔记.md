# Hadoop权威指南读书笔记
## 第一部分 Hadoop基础知识
### 第一章 初识Hadoop
- 不使用配置大量硬盘的数据库进行大规模数据分析，而是使用Hadoop，因为当今计算机磁盘的发展趋势：寻址时间的提升远远不敌于传输速率的提升
- RDBMS：适用于索引后数据集的点查询与更新，适合持续更新的数据集
- MapReduce：适合解决需要以批处理方式分析整个数据集的问题
- Hadoop对非结构化或半结构化数据非常有效，因为它在处理数据时才对其进行解释，提供灵活性的同时避免了RDBMS数据加载阶段带来的开销（只需文件拷贝）
- 关系型数据库往往是规范（Normalized）的，以保持数据的完整与不冗余，但对Hadoop而言，会使记录读取变得非本地化
- Web服务器日志是典型的非规范化数据（同一客户端全名可能多次出现），为Hadoop非常合适分析各种日志文件的原因之一

### 第二章 关于MapReduce
- Hadoop本身提供了一套可优化网络序列化传输的基本类型，而不直接使用Java的内嵌类型。这些类型位于org.apache.hadoop.io包中，如对应Long的LongWritable，对应String的Text，和对应Integer的IntWritable
- map()方法还提供Context实例用于输出内容的写入
- Job对象用于指定作业执行规范的流程：
    1. 在Hadoop集群上运行作业时，要把代码打包为一个jar文件以供Hadoop在集群上发布，但不必明确指定jar文件名称，通过Job对象的setJarByClass()方法传递一个类即可，Hadoop会利用其寻找包含它的jar文件
    2. FileInputFormat类的静态方法addInputPath()可以定义输入数据的路径，可以是单个文件或目录（此时，目录下所有文件都会被输入），或符合特定pattern的一系列文件。亦可通过多次调用此方法进行多路径输入
    3. FileOutputFormat类的静态方法setOutputPath()可用于指定reduce函数输出文件的写入目录。此目录在运行前不应存在，否则Hadoop会报错，目的是为了防止数据丢失（结果被意外覆盖）
    4. setMapperClass()和setReducerClass()方法用于指定map类与reduce类
    5. setOutputKeyClass()和setOutputValueClass()方法控制reduce函数的输出类型，且必须与reduce类产生的相匹配。map函数的输出类型默认与reduce函数相同，如若不同，则必须通过setMapOutputKeyClass()与setMapOutputValueClass()来设置输出类型
    6. 输入的格式通过InputFormat来控制，默认为TextInputFormat
    7. waitForCompletion()方法提交作业并等待执行完成，其返回的布尔值用于表示执行的成功与失败
- 如果调用Hadoop命令的第一个参数是一个类名，Hadoop会启动一个JVM来运行这个类，该命令会将Hadoop库（及其依赖关系）添加至ClassPath中，同时也能获取配置信息。为了将应用类添加到类路径，Hadoop定义了一个HADOOP_CLASSPATH环境变量，由Hadoop脚本执行相关操作
- 作业（job）：客户端执行的工作单元，包括输入数据、MapReduce程序和配置信息。Hadoop将job分成若干个task，包括两类：map task、reduce task，这些task运行在集群节点上，由Yarn进行调度，失败时会在另一个节点上重新运行
- Hadoop将MapReduce输入数据划分为等长的小数据块，称为数据分片（Input Split），Hadoop为每个分片构建一个map任务
- 分片被切分得更细时负载平衡的质量会更高，但管理分片的总时间和构建map task的开销会决定作业的整个执行时间。合理的大小趋向于一个HDFS块的大小（默认128MB，可针对集群调整新建文件大小或创建文件时单独指定），因为当分片横跨两个数据块时，同一个HDFS节点基本不可能同时存储两块数据
- Hadoop会在存储有输入数据的HDFS节点上运行map任务以提高数据本地化。但有时一个map任务的输入分片所处的HDFS节点可能正在运行其它任务，此时本地化会退化为同机架，甚至不同机架（非常偶然），因此会涉及数据的网络传输
- map任务将其输出的中间结果写入本地磁盘，并由reduce任务处理后产生最终结果，一旦完成作业，这些信息就可以删除。当中间结果传递失败时，Hadoop将在另一个节点上重新运行map任务
- reduce任务并不具备数据本地化优势，因为其输入通常来自所有mapper的输出，因此会涉及数据的网络传输。对于reduce输出的HDFS块，第一个副本会存储在本地节点上
- combiner通过Reducer类来定义，在job中通过setCombinerClass()方法设置，其属于优化方案，因此Hadoop无法确定要对一个指定的map任务输出记录调用多少次combiner，但不管调用多少次，reducer的输出结果都应是一样的

### 第三章 Hadoop分布式文件系统
- HDFS的构建思路：一次写入、多次读取是最高效的访问模式。每次分析都将涉及该数据集的大部分数据甚至全部，因此读取整个数据集的时间延迟比读取第一条记录的时间延迟更重要
- HDFS为高数据吞吐量应用优化，可能会以提高时间延迟为代价，因此要求低延迟数据访问的应用（例如几十毫秒范围）不适合在HDFS上运行
- HDFS的文件系统所能存储的文件总数受限于namenode的内存容量（每个文件、目录和数据块的存储信息大约占150字节）
- HDFS的文件写入只支持单个写入者，且是以“只添加”方式在文件末尾写数据
- HDFS的文件被划分为多个块（chunk），作为独立的存储单元。但与单磁盘文件系统不同，HDFS中小于块大小的文件不会占据整个块的空间
- namenode用于维护文件系统树及整棵树内所有的文件和目录。这些信息以两个文件形式永久保存在本地磁盘上：命名空间镜像文件和编辑日志文件
- namenode也记录每个文件中各个块所在的数据节点信息，但并不永久保存块的位置信息，因为系统启动时会根据数据节点信息重建
- datanode会定期向namenode发送自己存储的块的列表
- namenode的两种容错机制：
    1. 备份文件系统元数据持久状态文件，使namenode在多个文件系统上保存元数据持久状态，对它们的写操作实时同步且具有原子性。一般是通过远程挂载的网络文件系统（NFS）来完成
    2. 在另一台机器上运行一个辅助namenode，定期合并编辑日志与命名空间镜像，以防止编辑日志过大。它会保存合并后的命名空间镜像副本，并在namenode节点发生故障时启用，但由于信息的滞后性，主备转换时可能会丢失部分数据。在这种情况下，一般把存储在NFS的namenode元数据复制到辅助namenode并作为新的namenode运行
- 对于访问频繁的文件，其对应的块可能会被datanode显式缓存在内存中，以堆外块缓存的形式存在。用户或应用可以通过在缓存池（用于管理缓存权限和资源使用的管理性分组）中增加一个cache directive来告诉namenode需要缓存哪些文件及存多久
- 联邦HDFS：允许namenode通过分治文件系统命名空间的方式进行横向扩展，每个namenode维护一个命名空间卷（namespace volumn），由命名空间的元数据和一个数据块池组成，数据块池包含该命名空间下文件的所有数据块。卷之间相互独立不通信，单独失效也不影响其它。数据池块不再进行切分，集群中的datanode需要注册到每个namenode，并存储来自多个数据块池中的数据块
- 不启用HA时从失效的namenode中恢复：
    1. 启动一个拥有文件系统元数据副本的新namenode
    2. 将命名空间映像导入内存
    3. 重演编辑日志
    4. 接收到足够多的来自datanode数据块报告并退出安全模式
- Hadoop2引入的HA支持：
    - 配置了一对active-standby的namenode
    - namenode之间需要通过高可用共享存储实现编辑日志的共享，备用namenode接管后，会通读共享编辑日志直至末尾，并继续读取由活动namenode写入的新条目
    - datanode需要同时向两个namenode发送数据块处理报告，因为数据块映射信息存储于内存，而非磁盘
    - 客户端需要使用特定机制处理namenode的失效问题，且对用户透明
    - 辅助namenode的角色被备用namenode包含，其亦为活动namenode的命名空间设置周期性检查点
    - 可供选择的两种高可用共享存储：NFS或群体日志管理器QJM
    - QJM以一组日志节点的方式运行，每一次编辑必须写入多数日志节点（与ZooKeeper的工作方式类似）
    - 活动namenode失效后，备用namenode能快速（几十秒）实现任务接管，因为最新状态存储在内存中，包括最新的编辑日志条目和最新的数据块映射信息。实际观察到的时间会稍长（1分钟左右），因为系统需要保守确定活动namenode是否真的失效了
    - 活动namenode+备用namenode都失效的情况下（很罕见），依旧可以声明一个备用namenode并实现冷启动
    - 故障转移控制器：每个namenode都运行了一个轻量级的控制器，通过心跳机制监视宿主namenode是否失效，并在失效时进行故障切换（亦可被手动启用）
    - 在非平稳故障转移的情况下（如网络慢或被分割），同样也可能激发故障转移，但是先前的活动namenode仍然保持活动，这会导致其响应并处理客户过时的读请求（因为QJM同一时间仅允许一个namenode向编辑日志中写入数据）。因此需要设置一个SSH规避命令用于杀死namenode的进程
    - 对用户而言，可以通过配置客户端配置文件实现故障转移的控制：HDFS URI使用一个逻辑主机名，映射到一对namenode地址，客户端类库会访问每一个namenode直到处理完成
- Java接口部分暂时跳过
- 客户端通过远程调用访问namenode以获得文件块地址，namenode会返回存有该文件块副本的datanode地址，并且这些datanode会根据它们与客户端的距离进行排序（根据网络拓扑）
- 客户端调用read()方法时DFSInputStream会进行datanode的连接，并在到达块末端时关闭连接，并寻找下一个块最佳的datanode（用户无感知）
- 读取数据时若碰到datanode故障，DFSInputStream会从这个块的下一个最邻近datanode中读取数据，并记住故障datanode以保证以后不从它那读。如果发现损坏的块，则也会从其它datanode上读副本并将错误块通知给namenode
- 客户端写入数据时，DFSOutputStream会将它分为一个个数据包，并写入内部队列，DataStreamer负责处理数据队列，挑选出合适的一组datanode用于存储数据，并要求namenode进行数据块的分配。DataStreamer会将数据先发往第一个副本的datanode，该datanode存储数据包后会将其发送给第2个datanode，依此类推，写入成功后datanode会向DFSOutputStream发送ack，全部ack后才删队列
- 当写入数据过程中datanode故障时，关闭数据写入管线，将队列中所有数据包重新添加回队列最前端，以确保故障节点的下游datanode不漏数据。为存储在另一正常datanode的当前数据块指定一个新标识并传送给namenode，以便故障恢复后出错节点可以删除存储的部分数据块（？）。之后从管线中删除故障节点，并以剩余节点重建管线，完成写入。namenode注意到副本数不足时会在另一个节点上创建一个新的副本，后续数据块正常接受处理
- 块数据可以在集群中异步复制，直到达到目标副本数才算写入成功
- 调用close()时会将剩余数据包都写入datanode管线，并在联系namenode告知其文件写入完成前，等待确认（此时namenode已经知道文件由哪些块组成，只需等待数据块复制即可）
- **HDFS的一致模型**：当前正在写入的块对其它reader不可见，除非调用hflush()方法将所有缓存强制刷新到datanode中以使数据对新reader可见（hflush仅能保证数据到达节点内存中，不保证已落盘，因此无法避免断电带来的数据丢失，用hsync就可以保证落盘）。HDFS中的关闭文件隐含了hflush()方法
- distcp会使用MapReduce任务来完成文件的复制（仅需mapper），每个文件用一个mapper复制，并且distcp会尽量把每个map的数据大小分配得大致相等（把文件分块），可以通过-m把map数改为多于集群中节点数，以使数据分布均衡（第1个副本会写在map本地节点）。或使用balancer来改善集群中块的分布

### 第四章 关于YARN
- YARN通过两类长期运行的守护进程提供自己的核心服务：管理集群资源使用的Resource Manager、运行在集群所有节点上且能够启动和监控容器的Node Manager
- 一个容器可以是一个Unix进程，也可以是一个Linux cgroup，取决于YARN的配置
- YARN本身不会为应用的各部分（客户端、master和进程）彼此间通信提供任何手段。大多数重要的YARN应用使用某种形式的远程通信机制（如Hadoop的RPC）来向客户端传递状态更新和返回结果，这些机制专属于各应用
- 请求YARN容器时，可以指定每个容器需要的计算机资源数量及对容器的本地限制要求（可指定节点、机架，不满足时可选择放松限制）
- YARN可以在应用开始时提出所有资源申请请求（Spark），或动态地在需要资源时提出请求（MapReduce）
- YARN的应用生命期模型：
    - 一个用户作业对应一个应用：MapReduce采取的方式
    - 作业的每个工作流或每个用户对话（可能无关联）对应一个应用：容器可以在作业之间重用，并且有可能缓存作业之间的中间数据，是Spark采取的方式
    - 多个用户共享一个长期运行的应用：这种应用通常是作为一种协调者的角色，如Apache Slider，用于启动集群上的其它应用。Impala也通过这种方式提供了一个代理应用。一个总是开启的AM可以让查询响应延迟非常低
- YARN中，一个NodeManager管理一个资源池，而不是指定固定数目的slot（像MapReduce1一样）。因此不会出现集群中仅有map slot而导致reduce任务必须等待的情况。YARN中的资源是精细化管理的，可以按需请求资源
- YARN的调度机制
    - FIFO：
        - 没什么好说的
    - CapacityScheduler：
        - 通过维护一个专门的队列保证小作业一提交就可以启动，因此需要预留资源，降低集群资源利用率
        - 允许集群资源供多用户使用，每个用户维护一个队列，队列内任务按FIFO调度
        - 当队列需要的资源超出其容量时，调度器可能会从空闲资源里弹一部分资源给该队列（允许超出其容量）。空闲资源不足时也不会抢占，而是等待其它队列使用资源完毕。
        - 可以为队列设置最大容量限制以防止其过多侵占其它队列资源，但会牺牲自身弹性，因此需要在尝试中找到合理的折中
    - FairScheduler：
        - 会动态平衡资源（可指定权重，默认均分），但后启动的小作业需要等待第一个作业使用的容器用完并释放资源，等小作业结束后，资源会被先前的大作业拿回去
        - Fair的意思是每个用户队列间均分集群资源，而每个队列内部的任务只能均分队列资源（也可以把所有用户任务都堆到一个队列里来实现真正的共享）
        - 每个队列可以有自己的调度策略，默认是Fair
        - 调度器使用一个基于规则的系统（queuePlacementPolicy）来确定应用应该放在哪个队列中，每个规则会被一一尝试匹配，如是否有指定名字队列、用户的主Unix组名队列，等等
        - 抢占：可以设置最小共享抢占超时与公平共享抢占超时，时间均为秒级。前者会帮助任务在规定时间超过后还未得到资源时抢占资源，后者会帮助任务在规定时间超过后还未获得足够公平的资源时抢占资源，二者均可单独为队列配置或全体配置
- **延迟调度**：所有调度器都以本地请求为主，实践中往往无法满足本地化条件，但实践发现稍微等一下也许就能满足。Capacity和Fair都支持延迟调度。YARN通过nodemanager心跳返回的资源情况来驱动调度，启用延时后YARN会积攒调度机会次数，等达到次数上限时若还未能本地化，则再放宽本地化限制。亦可通过百分比来设置等待的节点数百分比（可在节点或机架层面进行设置）
- **主导资源公平性（DRF）**：当调度多种资源类型（如CPU、内存）时，YARN调度器会观察每个用户的主导资源，将其作为集群资源使用的一个度量。默认情况不使用DRF，便只考虑内存

### 第五章 Hadoop的I/O操作
- HDFS的数据完整性：
    - HDFS会对所有写入数据计算CRC-32C校验和，并在读取时验证校验和。可以指定对每多少量数据使用校验和（默认每512MB），校验和占4MB，造成的额外开销少于1%
    - 客户端写数据时，管线中最后一个datanode在收到数据与校验和后会先校验再存储。验证不通过时客户端会收到IOException
    - 客户端读数据时，会自行校验数据并与datanode中存储的校验和做对比，每个datanode会持久保存一个校验和日志（用于检测损坏磁盘），记录每个数据块最后的验证时间。客户端成功验证后会通知datanode，使其更新日志
    - 每个datanode自身也会定期用后台线程运行DataBlockScanner做校验
    - 客户端读数据块检测到错误时会向namenode报告损坏的数据块以及对应的datanode，再抛出ChecksumException。namenode会将其标记为已损坏，并不再将客户端请求发送至此节点，或尝试从其中复制数据到其它datanode。之后namenode会安排该数据块的副本在另一个datanode上完成复制以保证副本数，并删除损坏数据块副本
    - 调用open()方法读取文件前，可以把FileSystem对象的setVerifyChesksum()关上以禁用校验和验证，这样损坏的文件就不会被删了
    - hadoop fs -checksum命令可以检查文件校验和
    - Hadoop的LocalFileSystem执行客户端的校验和验证，文件系统客户端会在包含每个文件校验和的同一目录下新建一个.filename.crc隐藏文件。校验的文件块大小亦作为元数据存储在其中，所以即使校验文件块大小发生改变也能正确读回文件
    - 有的文件系统自带校验和功能，就可以禁用Hadoop的了（使用RawFileSystem取代LocalFileSystem）
    - ChecksumFileSystem：LocalFileSystem用其完成自己的任务。当LocalFileSystem读取数据发生错误时会将出错文件及其校验和移到同一设备下名为bad_file的边际文件夹（side directory）中
- 压缩
    - 常见的压缩格式：DEFLATE、gzip、bzip2（可切分）、LZO（预处理过程中若添加索引则可切分）、LZO4、Snappy
    - gzip、bzip2、lzo有自己的压缩工具，其余无。每个压缩工具都提供9个选项控制压缩时考虑的权衡，如-1为优化压缩速度、-9为优化压缩空间
    - gzip是一个通用的压缩工具，在空间/时间性能的权衡中位于另两位之间
    - bzip2压缩能力强于gzip但压缩速度慢些。bzip2的解压比压缩快，但仍慢于其它工具
    - LZO、LZ4和Snappy均优化压缩速度，比gzip快一个量级，但效率略逊。Snappy和LZ4的解压速度比LZO高出很多
    - Codec：代表对一个压缩-解压缩算法的实现，LZO代码库拥有GPL许可，需要单独下载
    - 为提高性能，最好使用native类库来实现压缩和解压缩。此情景下若应用需要大量执行压缩-解压缩操作，可以使用CodecPool来实现对象复用
    - MapReduce的输出格式可以设置RECORD级或BLOCK级的压缩
- 序列化
    - 序列化格式的四大理想属性：紧凑、快速、可扩展、支持多语言
    - Hadoop使用自己的Writable序列化格式
    - Hadoop的WritableComparator能直接比较数据流中的记录而无须将数据流反序列化为对象
    - 对整数进行编码时可以选择变长格式（VIntWritable和VLongWritable），以节省空间，且两者可以互相转换
    - Text类用于字符串编码，使用标准UTF-8编码（编码长度不固定），因此与String类之间存在一定差别，尤其是在迭代时
    - NullWritable用于存储空值，它并不占空间
    - 序列化类的实现需要在Serialization对象中注册
- 基于文件的数据结构
    - SequenceFile类为二进制键值对提供了一个持久数据结构，相比纯文件而言更适合用于日志记录，也可以作为小文件的容器，获得更高效率的存储和处理
    - SequenceFile在顺序写入数据时会插入一个特殊项以便每隔几个记录就有一个同步标识，以便数据读取迷路时能再次与记录边界同步
    - **可以将加入同步点的顺序文件用于MapReduce的输入，因为该类文件允许切分（SequenceFileInputFormat）**
    - SequenceFile可以启用记录级的压缩，但此时只有值被压缩，键不压。或启用块压缩，会利用记录间的相似性进行压缩，压缩效率更高且可追加数据，此时每个块的起始处都需要插入同步标识
    - MapFile是排序过的SequenceFile，有索引，支持按键查找。索引自身是一个SequenceFile，包含了map中一小部分键，能够加载进内存提供快速查找。主数据文件则是另一个SequenceFile，包含了所有条目，并已排序
    - 对MapFile进行写入时map条目必须顺序添加，否则会抛出IOException
    - Hadoop在MapFile上提供了一些变种：SetFile、ArrayFile、BloomMapfile（测试通过时才调用get()方法，对稀疏文件管用）
    - 顺序文件、map文件和Avro数据文件都是面向行的，Hadoop中面向列的格式有RC（被后两者取代）、ORC、Parquet。Avro也有面向列的文件格式Trevni

## 第二部分 关于MapReduce
### 第六章 MapReduce应用开发
- 多个资源文件可以按顺序通过addResource()方法添加到Configuration中，后添加的会覆盖前添加的属性，除非属性的final字段设置为true（此时会弹出警告）
- 通过System.setProperty()或JVM参数-Dproperty=value设置的属性优先级高于资源文件中定义的属性
- 系统属性需要使用配置属性重新定义，否则无法通过配置API进行访问
- 客户端的类路径由以下几个部分组成：
    - 作业的jar文件
    - 作业jar文件的lib目录中所有jar文件以及classes目录
    - HADOOP_CLASSPATH定义的类路径
- 集群上用户任务的类路径由以下几个部分组成：
    - 作业的jar文件
    - 作业jar文件的lib目录中所有jar文件以及classes目录
    - 使用-libjars或DistributedCache的addFileToClassPath()方法（老版本API）或Job（新版本API）添加到分布式缓存的所有文件
- 对于客户端和任务类路径，可以分别通过HADOOP_USER_CLASSPATH_FIRST环境变量和mapreduce.job.user.classpath.first参数来将用户的类路径优先放到搜索顺序中（默认在最后），以优先使用用户版本类库（会改变Hadoop框架依赖类，慎用）
- 作业历史文件由Application Master以JSON格式存放在HDFS中，会保存一周后删除
- 可以在作业中的自行添加Counter并在Context中对其进行操作以用于DEBUG
- 任务的中间结果文件可以通过参数将其保留。也可以通过YARN配置任务尝试文件的删前保留时间
- JobControl的实例表示一个作业的运行图，用户可以加入作业配置，并设置实例作业间的依赖关系

### 第七章 MapReduce的工作机制
- 作业的提交（客户端JobSumitter）：
    1. 向资源管理器请求一个新应用ID
    2. 检查作业的输出说明，若未指定或已存在则返回错误
    3. 计算作业的输入分片，若无法计算（如输入路径不存在）则返回错误
    4. 将运行作业需要的资源（包括jar、配置、计算获得的输入分片）复制到HDFS中以作业ID命名的目录下。作业jar的副本数较多（可由参数控制，默认为10），因此运行任务时集群中有多个副本供nodemanager访问
    5. 通过调用ResourceManager的submitApplication()方法提交作业
    6. 提交后，waitForCompletion()会每秒轮询作业的进度
- 作业的初始化：
    - RM收到submitApplication()请求后，将请求传递给YARN调度器，调度器分配一个容器，然后RM在对应NM的管理下在容器中启动AM
    - AM对作业的初始化是通过创建多个薄记对象（？）以保持对作业进度的跟踪来完成的
    - AM会接受来自HDFS的、在客户端计算的输入分片，并对每一个分片创建一个map任务对象，以及由参数配置的reduce任务对象。任务ID在此时分配
    - AM会决定如何运行各个任务，如果任务很小，可以直接运行在自己的JVM上。AM会判断在其它容器中分配与运行任务的开销是否会大于并行任务本身，若是，则会在一个节点上顺序运行任务，这样的任务称为Uber任务（需要明确用参数开启Uber任务）
    - 对小作业的定义：mapper数不多于指定值（默认10）且reducer不多于指定值（默认1）且输入大小小于指定值（默认HDFS块大小）
    - 在任务运行前AM调用setupJob()方法设置OutputCommiter（默认为FileOutputCommitter），表示将建立作业的最终输出目录及任务输出的临时工作空间
- 任务的分配：
    - 对于非Uber任务，AM会为其向RM请求容器，直到有5%的map任务已经完成时，reduce任务的请求才会发出
    - reduce任务能在集群中任意位置运行，但是map任务有数据本地化限制
    - 可以通过作业的计数器查看在每个本地化层次上运行的任务数量
    - 每个map任务和reduce任务分配的内存和cpu可以通过参数配置
- 任务的执行：
    - 一旦RM的调度器为任务分配了一个特定节点上的容器，AM就通过与NM通信来启动容器。该任务由主类为YarnChild的Java应用程序执行
    - 任务运行前会先将资源本地化，包括作业配置、jar文件和所有来自分布式缓存的文件
    - YarnChild在指定的JVM中运行，不会影响到NM
    - 每个任务都可以执行setup与commit，它们和任务本身在一个JVM中运行，并由作业的OutputCommiter确定。提交协议会确保当推测执行打开时只有一个任务副本被提交
    - Streaming运行特殊的map与reduce任务，目的是运行用户提供的可执行程序并与之通信
    - Streaming任务使用标准输入和输出流与进程进行通信，从NM的角度看就像子进程在自己运行map或reduce代码
- 进度的状态和更新：
    - 任务进度的定义：对于map而言，进度是已处理输入所占的比例。对于reduce而言，进度会复杂些，因为其拆成复制、排序、处理三个阶段（与shuffle的三个阶段对应），因此当任务已reduce完一半的输入时进度为5/6（1/3 + 1/3 + 1/2*1/3）
    - 任务计数器位于框架中，亦可由用户定义
    - map或reduce任务运行时，子进程会与自己的父AM通过umbiblical接口进行通信，每3秒报告一次进度与状态，AM会形成一个作业的汇聚视图（Aggregate View）
    - 作业完成时会将状态置为成功以便waitForCompletion()进行轮询，也可以通过设置让AM在完成时发送一个HTTP作业通知给指定域
    - 作业完成时AM和container会清理工作状态，如删除中间结果，OutputCommitter的commitJob()方法会被调用
- 失败：
    1. 任务失败
        - JVM在抛出异常退出前会向父AM发送错误报告，报告会被记入用户日志。AM将此次任务标记为failed并释放容器资源
        - 对于Streaming任务，如果Streaming进程以非0退出代码退出，则被标记为失败（可由参数控制）
        - 若JVM意外退出，则NM会注意到进程已退出，并通知AM将任务标记为失败
        - 若任务挂起使得AM一段时间（默认10分钟）没收到进度更新，也会被标记为失败，并且任务JVM会被杀掉
        - AM被告知任务失败后会重新调度该任务的执行并试图避免在以前失败过的NM上重新调度该任务，此外重试次数超过上限（默认值为4）后便不再重试，导致整个作业失败（可以通过参数设置在不触发作业失败的情况下允许map/reduce任务失败的最大百分比）
        - 任务是可以被Kill的（比如推测执行或NM故障），这与失败不同，不会被标记为任务失败
    2. AM失败
        - AM的尝试次数最多2次（可通过参数配置YARN层面及Mapreduce层面的最大尝试次数），超过后作业将失败
        - AM会向RM发送周期性心跳，因此AM失败时RM会感知失败并在新的容器中开始一个新的AM实例。对于MapReduce而言，新AM将使用作业历史来恢复失败的应用程序所运行任务的状态，使其不必重新运行（可以通过参数关闭AM的自动恢复）
        - MapReduce客户端会向AM轮询进度报告，AM的地址会在作业初始化期间由客户端向RM询问并缓存，若AM失败，则客户端会经历请求超时，并重新向RM询问并缓存AM地址，这个过程对用户透明
    3. NM失败
        - 如果NM由于崩溃或运行过于缓慢而失败，就会停止向RM发送心跳信息（或频率很低）。如果10分钟（可通过参数调整）内没有收到心跳信息，RM将会通知停止该NM并将其从节点池中移除
        - 失败的NM上运行的所有任务或AM会由前两节的机制进行恢复，未完成的作业会被AM安排重新运行，因为节点上的中间结果可能已无法访问
        - 应用程序运行失败次数过高时对应的NM会被拉黑，即使NM自己没失败过。黑名单由AM管理。对于MapReduce，如果一个NM上有超过三个任务（可通过参数调整）失败，AM就会将任务尽量调度到不同节点上
    4. RM失败
        - RM失败是非常严重的事情，作业和容器无法启动且单点故障的情况下所有运行中的作业都会失败且无法恢复
        - 为获得HA，在双机热备配置下运行一对RM是必要的
        - 所有运行中的应用程序信息存储在一个HA的状态存储区中（Zookeeper或HDFS）
        - NM的信息不会被存储在状态中，因为可以直接由第一次心跳中的信息重构
        - 任务的信息亦不会被存储在状态中，因为它们由AM管理
        - 新RM启动后会从状态存储区读取应用程序信息并为集群中运行的所有应用程序重启AM，此次AM重启不会算进AM的失败次数
        - RM从备机到主机的切换由故障转移控制器（failover controller）处理，默认是自动工作的，由ZooKeeper的选举机制确保同一时刻只有一个主RM
        - 不同于HDFS的HA实现，故障转移控制器不必是一个独立的进程，为配置方便，默认情况下嵌入在RM中，其也可以配置为手动处理，但不建议这么搞
        - 客户端与NM需要进行相应配置，以在主备RM中轮询以找出主RM
- shuffle与排序
    - MapReduce会确保reduce的输入都是按键排序的，系统执行排序、将map输出作为reduce输入传给reduce的过程称为shuffle
    - map端：
        - map函数开始产生输出时会先利用缓冲的方式写到内存进行预排序
        - 每个map任务都有一个环形内存缓冲区用于存储任务输出，默认情况下大小为100MB（可由参数调整），一旦缓冲区达到阈值（默认80%），便会由一个后台线程将内容溢出到磁盘
        - 写盘过程中map输出会继续写到缓冲区，若缓冲区被填满则输出被阻塞直到写盘完成
        - 写盘过程按轮询方式将内容写到参数指定的目录下
        - 写盘前线程会先根据数据最终要传的reducer将数据划分为相应的分区并对每个分区按键进行内存中排序。如果有combiner函数，它会在排序后的输出上运行，可以减少写盘及发送给reducer的数据
        - 在任务完成之前，所有溢出的文件会被合并为一个已分区且已排序的输出文件（可由参数配置一次最多能合并多少流）
        - 如果存在至少3个溢出文件（可由参数配置值），combiner会在最终输出文件写盘前再次运行（因其幂等性）
        - map输出写入到磁盘前可以选择将文件压缩
        - reducer通过HTTP得到输出文件的分区，用于文件分区的工作线程数量可由参数控制，此设置针对NM，而非map任务。默认值0的意义是CPU数量的两倍
    - reduce端：
        - map任务完成后会使用心跳机制通知AM，AM知道该作业中map输出与主机位置间的映射关系，reducer中的一个线程会定期来询问AM以获取输出位置，直到获得所有输出位置
        - reduce任务会在每个map任务完成时使用少量复制线程（默认为5个）并行地去map节点本地磁盘上复制其输出
        - 如果map输出小，会直接被复制到reduce任务的JVM内存中，若大，则复制到磁盘中（内存缓冲区可以由参数设置）
        - 内存缓冲区达到阈值或达到map输出阈值（可分别由参数设置）时，将合并溢写至磁盘中。若指定了combiner，则在合并期间会运行它以降低磁盘写入数据量
        - 随着磁盘上副本数量增多，后台线程会将它们合并为更大的排序文件。合并过程中map的压缩输出会在内存中被解压
        - 复制完map输出后，reducer进入合并排序阶段。假如有50个Map输出，而合并因子是10（可配置），合并将进行5趟，每趟合并10个文件，最终生成5个中间文件（实际的每趟合并大小会有所优化，目标是合并最小数量的文件以便满足最后一趟的合并系数）
        - reduce阶段会对排序输出中的每个键调用reduce函数，并直接输出到文件系统（一般为HDFS，且第一个副本会被写进本节点）
- 配置调优：尽量给shuffle足够内存，因此不建议在map或reduce函数中使用过大内存。在map端尽量减少溢写文件次数（会被记数器记录，但其值包括了map与reduce两端的溢写）。在reduce端尽量把中间数据放入内存中
- 推测执行：可以分别为map和reduce任务配置是否开启，默认打开。以牺牲集群吞吐量的前提减少作业执行时间。对于reduce任务关闭是有益的，可以避免shuffle阶段的网络传输。非幂等任务也别开了，免得结果不对
- OutputCommitters：
    - 在作业运行前在setupJob()中设置，默认为FileOutputCommiter
    - FileOutputCommitter在初始化时会创建最终输出目录并在其下创建临时工作目录_temporary
    - 当任务成功时调用commitJob()方法，将临时工作目录删除并创建_SUCCESS隐藏文件
    - 当任务失败时调用abortJob()方法，默认将临时工作空间整个删除
    - 任务级别上也可以做此操作，在setupTask()中设置（默认什么也不做），任务的提交阶段是可选的，此时框架不会为其运行分布式提交协议，打开时框架会保证特定任务有多个attempt的情况下只有一个会成功提交
- 可以设置将map任务和reduce任务的输出直接写到分布式文件系统如HDFS上，以实现比单个键值对更大的灵活性

## 第八章 MapReduce的类型与格式
- 由于Java编译期的泛型擦除机制，map和reduce任务的输入/输出类型必须显式设置，并且不兼容类型只有在运行时才会被检测出来（实践中可先以少量数据跑一次任务用以检测）
- Streaming相关的东西暂时跳过
- 输入格式
    - 一个map操作只处理一个输入分片
    - 输入分片包含若干个记录，分片与记录都是逻辑概念，仅为指向数据的引用
    - 分片会被排序，大分片会被优先处理以最小化作业运行时间
    - FileInputFormat是所有使用文件作为数据源的InputFormat实现类的爹。它提供两个功能：指出作业的输入文件位置、为输入文件生成分片。把分片分割成记录的操作由子类完成
    - FileInputFormat中作业的输入被设定为一组路径，一条路径可以表示一个文件、一个目录或一个glob（文件和目录的集合）。可以使用setInputPathFilter()方法设置一个过滤器（默认会排除.和_开头的隐藏文件，这是强制性的）
    - CombineFileInputFormat可以把多个小文件打包到一个分片中以减少过多的map数，并且在打包时会考虑节点和机架的因素
    - 减少大量小文件的其中一个办法是用Sequence File将这些小文件合并成一个或多个大文件
    - 对于不希望文件被切分的特定任务，可以通过将分片大小改大或继承FileInputFormat类并将isSplitable()方法重写为返回false
    - mapper可以通过调用Context.getInputSplit()方法获取分片信息
    - **当分片中数据的边界与HDFS块的边界没对齐时，会出现本地数据不全的情况从而引入网络数据传输**
    - TextInputFormat的键是该行数据的字节偏移量，而非行号，因此mapper收到的输入行数会不同。若希望行数相同，可使用NLineInputFormat，它的数据格式相同，只是分片的构造方式不同
    - 当使用文本输入格式时可以设置最大接受行长度以避免文件损坏（通常表现为超长行）而导致内存溢出
    - MultipleInputs类允许为每条输入路径指定InputFormat和Mapper，在addInputPath()时作为参数传入
- 输出格式
    - MapFileOutputFormat把MapFile（排好序的SequenceFile）作为输出，但MapFile中的键必须顺序添加，因此必须确保reducer输出的键已经排好序
    - MultipleOutputFormat可以对输出文件名进行控制或让每个reducer输出多个文件，通过调用MultipleOutputs的write()方法
    - LazyOutputFormat可以等指定分区第一条记录输出时才真正创建输出文件（FileOutputFormat的子类会直接产生输出文件，即使是空的）

## 第九章 MapReduce的特性
- 计数器
    - Hadoop为每个作业维护若干计数器，以描述多项指标，它们的分组为：
    1. 任务计数器：
        - 由其关联任务维护，并定期发送给AM
        - 每次传输完整值，而非增量，以避免由于消息丢失引发错误
        - 若任务失败，则相应计数器值会减少
    2. 作业计数器：
        - 由AM维护，因此无需在网络间传输数据
        - 作业级别的统计量，值不会随着任务运行而改变
    3. 用户定义的Java计数器
        - 可在mapper或reducer中增加，由Java枚举类型定义，以便进行分组（枚举类名即为组名，字段名即为计数器名）
        - 计数器是全局的，框架会跨所有map和reduce做收集，作业结束时产生最终结果
        - 可以使用getCounter()的String入参重载方法来动态地创建计数器
        - Job对象的getCounter()可以返回该作业的所有计数器
    4. 用户定义的Streaming计数器
        - 使用Streaming的MapReduce程序可以向标准错误流发送一行特殊格式的信息来增加计数器的值
- 排序
    - 键的排列顺序由RawComparator控制，规则为：
        1. 若Comparator Class由参数或setSortComparatorClass()方法显式设置，则使用该类的实例
        2. 否则，键必须是WritableComparable的子类，并使用针对该键类的已登记的comparator
        3. 如果没有已登记的comparator，则使用RawComparator，将字节流反序列化为一个对象，再由WritableComparable.compareTo()方法做比较
    - 全排序：
        - 简单粗暴的方法是使用单个reduce任务，低效
        - 另一种高效的方法是将数据以键范围进行分片，这样每个范围内排好序后将所有分片做拼接即可。但有可能导致数据不均匀，可以通过客户端数据采样解决（InputSampler）
    - 辅助排序：
        - 实现按值排序：定义包括自然键和自然值的组合键，根据组合键对记录进行排序，针对组合键进行分区和分组时只考虑自然键
- 连接（Reduce端）
    - Reduce端连接的输入源往往有多种格式，可以使用MultipleInputs类来方便地解析和标注各个源
    - Reducer会从不同源中选出键相同的记录，但这些记录不保证为排序记录。实践中往往需要一个源的数据排列在另一个源的数据前面以更好地执行连接（小表在前）。可以通过上一节的辅助排序达到目标
- 边数据分布
    - 边数据（side data）是作业所需的额外只读数据，以辅助处理主数据集。面临的主要挑战是如何使散布在集群中的所有map和reduce任务都能够方便且高效地访问这些数据
    1. 利用JobConf来配置作业
        - Configuration类的各种setter方法可以方便地配置作业的任一键值对，对任务传递少量元数据时非常管用
        - 任务中用户可以通过Context.getConfiguration()方法获得配置信息
        - 对于复杂对象需要用户自己实现对象与字符串之间的双向转换机制，或使用Hadoop提供的Stringifier类。这部分会加大MapReduce组件的内存压力，因此数据量不宜过大
        - 作业配置总是由客户端、AM和任务JVM读取，所有项会被读到内存中（包括不用的）
    2. 分布式缓存
        - 能够在任务运行过程中及时地将文件和存档复制到任务节点以供使用，为节约带宽，各个文件通常只需要复制到一个节点一次
        - 工作机制：当用户启动一个作业，Hadoop会把由-file、-archives、-libjars等选项（亦可使用分布式缓存API）指定的文件复制至分布式文件系统中，并在任务运行之前由NM将文件从分布式文件系统复制到本地磁盘使其本地化（以符号链接的方式指向任务的工作目录）。-libjars指定的文件还会在任务启动前添加到任务的CLASSPATH中
        - NM会为节点中缓存的文件维护一个计数器来统计其使用情况，任务即将运行时相关的文件对应的计数器+1，执行完毕后再-1。当计数达到0时才有资格删除。当缓存容量超出上限（默认10GB）时会以LRU方式腾出空间装载新文件
        - **该机制不保证同一节点上运行的同一作业的后续任务肯定能在缓存中找到文件，但是成功的概率很大。因为作业的多个任务在调度后几乎同时开始运行，不会有足够多的其它作业在运行而导致原始任务的文件从缓存中被删除**
        - 缓存中可以存放两类对象：文件和存档，前者会直接放入任务节点，后者会先解包再放入任务节点
- MapReduce库类
    - Hadoop还为mapper和reducer提供了一个包含常用函数的库，比如
    - ChainMapper, ChainReducer：在一个mapper中运行多个mapper，在一个reducer中运行多个reducer，以降低磁盘IO开销
    - FieldSelectionMapReduce：能从输入键和值中选择字段并输出键和值的mapper和reducer
    - IntSumReducer, LongSumReducer：对各键的所有整数值执行求和操作的reducer
    - InverseMapper：交换键和值的mapper
    - MultithreadedMapper：能在多个独立线程中并发运行mapper的mapper，使用CPU牛逼的mapper的使用
    - TokenCountMapper：将输入值用Java的StringTokenizer分解成独立单词并输出每个单词及count值1的mapper
    - RegexMapper：检查输入值是否匹配某正则表达式，输出匹配字符串和count值1的mapper